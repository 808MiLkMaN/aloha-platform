# ğŸŒº Aloha Platform - Auto-Install Complete Setup

## âœ¨ ONE-COMMAND AUTO-INSTALL

```bash
# Windows PowerShell (Run as Administrator)
powershell -ExecutionPolicy Bypass -File .\scripts\auto-install.ps1

# macOS / Linux
bash ./scripts/auto-install.sh
```

---

## ğŸ“¦ What Gets Installed

### System Components
- âœ… Node.js 25+ (if needed)
- âœ… Ollama (Local LLM runtime)
- âœ… Git (if needed)
- âœ… PM2 (Process manager)

### AI Models
- âœ… Mistral 7B (primary model)
- âœ… Neural-Chat 7B (alternative)
- âœ… Llama2 7B (optional)

### Configuration
- âœ… .env file setup
- âœ… Token initialization
- âœ… Admin credentials (malcolm@gmail.com)
- âœ… LLM API configuration

### Services Started
- âœ… Next.js dev server (port 3000)
- âœ… Ollama service (port 11434)
- âœ… Admin dashboard ready

---

## â±ï¸ Installation Time

- **First Run:** 15-30 minutes (includes model downloads)
- **Subsequent Runs:** 2-5 minutes
- **Model Download Time:** 7-15 minutes (depends on internet speed)

---

## ğŸš€ Quick Start

### 1. Run Auto-Install

**Windows:**
```powershell
cd C:\Users\Ekolu\aloha-platform
powershell -ExecutionPolicy Bypass -File .\scripts\auto-install.ps1
```

**macOS/Linux:**
```bash
cd ~/aloha-platform
bash ./scripts/auto-install.sh
```

### 2. Access Application

Open browser: **http://localhost:3000**

### 3. Test LLM API

```bash
curl -X POST http://localhost:3000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello!"}'
```

---

## ğŸ“‹ Installation Checklist

After running auto-install, verify:

- [ ] Terminal shows "âœ… Installation Complete"
- [ ] Ollama running on http://localhost:11434
- [ ] Next.js running on http://localhost:3000
- [ ] Models downloaded and available
- [ ] Admin tokens generated
- [ ] .env file configured
- [ ] No error messages

---

## ğŸ†˜ Troubleshooting

### Ollama Won't Start

```bash
# Try manual start
ollama serve

# In another terminal, test
curl http://localhost:11434/api/tags
```

### Models Won't Download

```bash
# Check internet connection
ping google.com

# Manual download
ollama pull mistral
ollama pull neural-chat
```

### Port Already in Use

```bash
# Find and kill process on port 3000
# Windows: Use Task Manager (End Task for node.exe)
# macOS/Linux: lsof -i :3000 | grep LISTEN | awk '{print $2}' | xargs kill -9

# Or change port
PORT=3001 npm run dev
```

---

## ğŸ” Admin Credentials

After installation:

**Email:** `malcolmlee3@gmail.com`
**Access Level:** `LIFETIME_ENTERPRISE` (Unlimited)
**Token File:** `.admin-tokens.json`

Use these credentials to access admin dashboard.

---

## ğŸ“Š System Requirements

- **OS:** Windows 10+, macOS 10.14+, Ubuntu 18.04+
- **RAM:** 8GB minimum (16GB recommended for models)
- **Disk:** 50GB free (for models)
- **CPU:** 4 cores minimum
- **Internet:** Stable connection for model downloads

---

## ğŸ¯ What's Included

### Frontend
- âœ… Next.js 16 development server
- âœ… React 19 components
- âœ… Tailwind CSS styling
- âœ… Beautiful dashboard UI

### Backend
- âœ… Token management API
- âœ… LLM generation API
- âœ… Multi-provider support
- âœ… Authentication routes

### Local LLM
- âœ… Ollama integration
- âœ… Mistral model (7B)
- âœ… Streaming support
- âœ… Model management

### Tools
- âœ… PM2 process manager
- âœ… Admin token generator
- âœ… Setup automation
- âœ… Environment configurator

---

## ğŸ“ Post-Installation Steps

### 1. Verify Installation

```bash
# Check dev server
curl http://localhost:3000

# Check LLM service
curl http://localhost:11434/api/tags

# Check admin tokens
cat .admin-tokens.json
```

### 2. Test API Endpoints

```bash
# Generate text with Mistral
curl -X POST http://localhost:3000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is machine learning?",
    "provider": "local",
    "model": "mistral"
  }'

# Get admin tokens
curl -X POST http://localhost:3000/api/auth/tokens/generate \
  -H "Content-Type: application/json" \
  -d '{"email": "admin@example.com"}'
```

### 3. Monitor Services

```bash
# View all services
pm2 list

# View logs
pm2 logs

# Monitor real-time
pm2 monit
```

### 4. Update Environment (Optional)

Edit `.env` to customize:

```env
LLM_PROVIDER=local
LOCAL_LLM_API_URL=http://localhost:11434
LOCAL_LLM_MODEL=mistral
NODE_ENV=development
PORT=3000
```

---

## ğŸš€ Deployment Options After Setup

### Option 1: Keep Running Locally
```bash
npm run dev
# Runs indefinitely for development
```

### Option 2: Deploy to VM
```bash
# Follow: VM-HOSTING-SETUP.md
bash /path/to/deployment-script.sh
```

### Option 3: Docker Deployment
```bash
docker-compose up -d
# All services in containers
```

---

## ğŸ“š Additional Resources

- **Documentation:** `/docs`
- **LLM Guide:** `lib/llm-service.ts`
- **Token Manager:** `lib/token-manager.ts`
- **API Routes:** `src/app/api/`
- **Quick Start:** `DEPLOYMENT-QUICK-START.md`
- **VM Setup:** `VM-HOSTING-SETUP.md`

---

## ğŸ‰ You're Ready!

After installation:
1. âœ… Frontend running on http://localhost:3000
2. âœ… LLM API ready at /api/llm/generate
3. âœ… Admin tokens in .admin-tokens.json
4. âœ… Local LLM models downloaded
5. âœ… Ready for development or deployment

---

**ğŸŒº Installation Complete! Happy coding! ğŸš€**
